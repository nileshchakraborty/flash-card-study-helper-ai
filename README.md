# Flash Card Study Helper AI

ğŸ“š **Overview**
AIâ€‘powered flashâ€‘card study application with swipeable cards, file uploads, and interactive quizzes.

## Features
- **Swipeable Flashcards** â€“ Tinderâ€‘like swipe interface for studying
  - Swipe left (or click â€œReviseâ€) to put cards back in the deck
  - Swipe right (or click â€œNextâ€) to mark cards as mastered
  - Works on both mobile (touch) and desktop (mouse/buttons)
- **Topicâ€‘Based Generation** â€“ Generate flashcards from any topic using AI
  - Enter a topic and number of cards
  - AI generates relevant flashcards automatically
- **File Upload** â€“ Convert PDFs and images into flashcards
  - Upload PDFs, PNG, JPG, GIF files
  - Drag and drop support
  - Automatic text extraction and flashcard generation
- **Interactive Quiz** â€“ Test your knowledge
  - Generate quizzes from your flashcards
  - Answer questions and get instant feedback
  - Review correct and incorrect answers
- **Study Plan** â€“ AIâ€‘generated study plans based on your progress
  - Automatically creates daily study plans
  - Tracks left/right swipes for revision planning
  - Recreates plans based on your learning progress

> **Note:** The UI now automatically detects when **â€œEnable Offline AIâ€** (WebLLM) is active and uses the appropriate runtime â€“ no manual checkbox needed.

---

## ğŸš€ Getting Started

### Prerequisites
- **Node.js**â€¯â‰¥â€¯14  
- **Ollama** (for local LLM) â€“ optional if you prefer WebLLM only  

### Installation
```bash
# 1ï¸âƒ£ Clone the repo
git clone [https://github.com/your-repo/flash-card-study-helper-ai](https://github.com/your-repo/flash-card-study-helper-ai)
cd flash-card-study-helper-ai

# 2ï¸âƒ£ Install dependencies
npm install

# 3ï¸âƒ£ Set up environment variables
cp .env.example .env
# Edit .env â†’ set OLLAMA_BASE_URL, OLLAMA_MODEL, SERPER_API_KEY, etc.

# Ollama Setup (optional)
```bash
# Install Ollama (https://ollama.ai)
ollama pull llama3.2   # or any Ollamaâ€‘compatible model
```

# Run the Application
```bash
npm start
# Open http://localhost:3000 in your browser
```

### Development
```bash
npm run demo   # runs a quick demo of core functionality
npm run build  # builds the frontend (esbuild)
```

---

## ğŸ¯ Usage

### 1ï¸âƒ£ Create Flashcards
- Create Cards tab â†’ enter a topic and card count.
- Enable Offline AI (bottomâ€‘right) to use WebLLM; otherwise Ollama is used.
- Click Generate Flashcards â†’ cards appear in the Study tab.

### 2ï¸âƒ£ Deepâ€¯Dive Mode
- After finishing a deck, select Deepâ€¯Dive (radio button).
- Click Move to Harder Questions â†’ the system generates advanced cards based on the current topic.

### 3ï¸âƒ£ Quiz
- Quiz tab â†’ set number of questions â†’ answer and review results.

### 4ï¸âƒ£ Study Plan
- The app builds a daily study plan based on your swipe history.

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
| --- | --- | --- |
| GET | /api/flashcards | Retrieve all flashcards |
| POST | /api/flashcards | Add flashcards manually |
| POST | /api/upload | Upload PDFs/images for conversion |
| POST | /api/generate | Generate flashcards (supports `runtime: 'ollama'` and `runtime: 'webllm'`) |
| GET | /api/quiz?size=5 | Generate a quiz |
| POST | /api/quiz/grade | Grade quiz answers |
| POST | /api/swipe | Record swipe action |
| GET | /api/swipe-history | Swipe statistics |
| GET | /api/study-plan | Generate study plan |
| POST | /api/reset | Reset the deck |
| GET | /api/health | Health check (Ollama & Serper) |

Example â€“ Ollama generation

```bash
curl -X POST http://localhost:3000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"topic":"React Hooks","count":3,"runtime":"ollama","knowledgeSource":"ai-only"}'
```

Example â€“ WebLLM generation

```bash
curl -X POST http://localhost:3000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"topic":"Kubernetes","count":2,"runtime":"webllm","knowledgeSource":"ai-web"}'
```

---

## ğŸ¤– AI Integration

### Ollama (Serverâ€‘side)
Generates flashcards, summaries, search queries, and deepâ€‘dive content.
Configurable via .env (OLLAMA_BASE_URL, OLLAMA_MODEL).

### WebLLM (Browserâ€‘side)
Runs entirely in the browser when Enable Offline AI is active.
The UI now automatically selects runtime: 'webllm' based on the ModelManagerUI state.

### Knowledge Sources

- ai-only â€“ Pure LLM generation.
- web-only â€“ Web search only (no LLM).
- ai-web â€“ Combined LLM + web search (default).

---

## ğŸ“Š Metrics Service

All generation attempts are logged to .metrics/generations.jsonl with:

```json
{
  "runtime":"ollama|webllm",
  "knowledgeSource":"ai-only|web-only|ai-web",
  "mode":"standard|deep-dive",
  "topic":"Your Topic",
  "cardCount":5,
  "duration":1234,
  "success":true,
  "timestamp":1763942894820
}
```

Metrics are loaded on server start and used for analytics & future model training.

---

## ğŸ› ï¸ Project Structure

```bash
flash-card-study-helper-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ secondary/
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama/          # Ollama adapter
â”‚   â”‚   â”‚   â””â”€â”€ serper/          # Web search adapter
â”‚   â”‚   â””â”€â”€ primary/
â”‚   â”‚       â””â”€â”€ express/         # API server
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ ports/               # Interfaces
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ StudyService.ts  # Orchestrates generation & deepâ€‘dive
â”‚   â”‚       â””â”€â”€ MetricsService.ts# Tracks generation metrics
â”‚   â””â”€â”€ index.ts                 # Application entry point
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ api.service.ts          # Wrapper for API calls
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LLMOrchestrator.ts   # Handles WebLLM model loading
â”‚   â”‚   â”‚   â””â”€â”€ ConfigurationService.ts # (now deprecated â€“ runtime autoâ€‘detect)
â”‚   â”‚   â””â”€â”€ views/
â”‚   â”‚       â”œâ”€â”€ generator.view.ts       # UI for card generation
â”‚   â”‚       â””â”€â”€ quiz.view.ts
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .metrics/                    # JSONL logs
â”œâ”€â”€ .env & .env.example
â””â”€â”€ README.md
```

---

## ğŸ“¦ Future Enhancements (Roadmap)

- Realâ€‘time AI service integration (e.g., OpenAI, Anthropic)
- User authentication & data persistence
- Spacedâ€‘repetition algorithm
- Export / import flashcards
- Multiple decks & sharing
- Advanced analytics & progress dashboards
- Quiz Mode with AI grading 
- Metrics service for tracking generation metrics
- Backend API with health checks

---

## ğŸ“ License

MIT

---

## ğŸ“š Quick Reference â€“ Enabling Offline AI

1. Click Enable Offline AI (bottomâ€‘right).
2. The UI now automatically sets runtime: 'webllm' for the next generation request.
3. No manual checkbox is needed â€“ the change is handled in 
generator.view.ts by checking llmOrchestrator.isModelLoaded().


Enjoy building smarter study sessions! ğŸ“âœ¨
