# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Serper API Configuration (for web search-based flashcard generation)
# Get your free API key from https://serper.dev
SERPER_API_KEY=your_serper_api_key_here

# JWT/JWE Secret Key (IMPORTANT for Production!)
# This key is used to encrypt/decrypt authentication tokens
# If not set, a random key will be generated, which works for development
# but MUST be set to a consistent value in production environments
# Generate a secure key: node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"
JWE_SECRET_KEY=your_secure_32_char_secret_key_here

# Server Configuration
PORT=3000

# Debug Mode (optional)
# Set to 'true' to enable debug logging for Ollama responses
DEBUG_OLLAMA=false

# Cache Configuration
CACHE_SERPER_TTL_SECONDS=3600
CACHE_SERPER_MAX_ENTRIES=100
CACHE_LLM_TTL_SECONDS=86400
CACHE_LLM_MAX_ENTRIES=500


NEXT_PUBLIC_SUPABASE_ANON_KEY="some-example-anon-key"
SUPABASE_SERVICE_USER_KEY="some-service-user-key"
NEXT_PUBLIC_SUPABASE_URL="https://${SUPABASE_SERVICE_USER_KEY}.supabase.co"
POSTGRES_DATABASE="postgres"
POSTGRES_HOST="db.${SUPABASE_SERVICE_USER_KEY}.supabase.co"
POSTGRES_PASSWORD="postgres_password_example"
POSTGRES_PRISMA_URL="postgres://postgres.${SUPABASE_SERVICE_USER_KEY}:${POSTGRES_PASSWORD}@aws-1-us-east-1.pooler.supabase.com:6543/postgres?sslmode=require&pgbouncer=true"
POSTGRES_URL="postgres://postgres.${SUPABASE_SERVICE_USER_KEY}:${POSTGRES_PASSWORD}@aws-1-us-east-1.pooler.supabase.com:6543/postgres?sslmode=require&supa=base-pooler.x"
POSTGRES_URL_NON_POOLING="postgres://postgres.${SUPABASE_SERVICE_USER_KEY}:${POSTGRES_PASSWORD}@aws-1-us-east-1.pooler.supabase.com:5432/postgres?sslmode=require"
POSTGRES_USER="postgres"
SUPABASE_ANON_KEY="some-anon-key"
SUPABASE_JWT_SECRET="some-jwt-secret"
SUPABASE_SERVICE_ROLE_KEY="some-service-role-key"
SUPABASE_URL="https://${SUPABASE_SERVICE_USER_KEY}.supabase.co"

AI_GATEWAY_KEY="some-ai-gateway-key"
REDIS_URL="redis://some-redis-url"

# ===== Docker-based Local Databases =====
# Use these when running `docker compose up`

# PostgreSQL (Local Docker)
# DATABASE_URL="postgresql://flashcard:flashcard_dev@localhost:5432/flashcard_db"

# Neo4j Graph Database (Local Docker)
NEO4J_URI="bolt://localhost:7687"
NEO4J_USER="neo4j"
NEO4J_PASSWORD="flashcard_dev"

# Qdrant Vector Database (Local Docker alternative to Upstash)
# QDRANT_URL="http://localhost:6333"

# Redis (Local Docker - for caching and BullMQ queues)
# REDIS_HOST="localhost"
# REDIS_PORT="6379"

# Use local queue instead of Redis-based BullMQ (for dev without Redis)
USE_LOCAL_QUEUE="true"

UPSTASH_VECTOR_REST_READONLY_TOKEN="some-upstash-vector-rest-readonly-token"
UPSTASH_VECTOR_REST_TOKEN="some-upstash-vector-rest-token"
UPSTASH_VECTOR_REST_URL="vector-db-url"
DEEPSEEK_API_KEY="some-deepseek-api-key"

BLOB_READ_WRITE_TOKEN="some-blob-read-write-token"

GOOGLE_CLIENT_ID="some-google-client-id"
GOOGLE_CLIENT_SECRET="some-google-client-secret"

# ===== LLM Provider Configuration =====
# Configure which AI providers to use for flashcard/quiz generation

# Provider Priority (comma-separated, first available will be used)
# Options: ollama,anthropic,google,custom
# NOTE: webllm is CLIENT-SIDE ONLY and should NOT be in backend priority
# Default: ollama (backend priority excludes webllm)
LLM_PRIORITY="ollama"

# Default provider (fallback if priority list fails)
# Default: ollama
LLM_DEFAULT_PROVIDER="ollama"

# --- Ollama Configuration (Self-hosted or Custom Endpoint) ---
# URL to your Ollama instance (no /api/ suffix needed)
# Examples:
#   Local: http://localhost:11434
#   Remote: https://your-ollama-server.com
#   Cloud: https://ollama-api.example.com
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="llama3.2:latest"

# --- Anthropic (Claude) Configuration ---
# Get API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=""
ANTHROPIC_MODEL="claude-3-5-sonnet-20241022"

# --- Google (Gemini) Configuration ---
# Get API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=""
GOOGLE_MODEL="gemini-1.5-pro"

# --- Custom OpenAI-compatible API ---
# Use for Groq, Together, OpenRouter, or any OpenAI-compatible endpoint
# Examples:
#   Groq: https://api.groq.com/openai/v1
#   Together: https://api.together.xyz/v1
#   OpenRouter: https://openrouter.ai/api/v1
CUSTOM_LLM_URL=""
CUSTOM_LLM_API_KEY=""
CUSTOM_LLM_MODEL="llama-3.1-70b-versatile"

# --- WebLLM Configuration ---
# WebLLM runs client-side in the browser - no configuration needed!
# Always available as fallback option
# Note: WEBLLM_ENABLED is handled by frontend settings

# ===== OAuth Configuration =====

# OAuth Callback URL (Optional)
# If not set, will derive from request headers (recommended for preview deployments)
# For production, set to your production domain:
# OAUTH_CALLBACK_URL="https://mindflipai.vercel.app/api/auth/google/callback"
# For local development, leave unset to use http://localhost:3000


# Test Auth Token for API Testing
# Generated: 2025-12-06
# This token represents a test user and can be used for development/testing
# Token expires in 2 hours from generation

TEST_AUTH_TOKEN="test-auth-token"

# Usage:
# curl -X POST http://localhost:3000/api/generate \
#   -H "Authorization: Bearer $TEST_AUTH_TOKEN" \
#   -H "Content-Type: application/json" \
#   -d '{"topic":"Machine Learning","count":2}'

# Test user details:
# - ID: test-user-123
# - Email: test@example.com  
# - Name: Test User

# To regenerate: npx tsx scripts/generate-test-token.ts